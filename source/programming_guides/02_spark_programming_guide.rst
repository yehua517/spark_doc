spark 编程指南
=============

* 概述
* 使用saprk
* 初始化Spark
	* 使用spark-Shell
* 弹性分布式数据集 (RDDs)
	* 并行化集合
	* 外部数据集
	* RDD操作
		* 基本操作
		* 函数操作
		* 理解闭包
			* 例子
			* 本地模式 VS 集群模式
			* 输出RDD的元素
		* 使用key-value键值对(tuple)
		* Transformations
		* Actions
		* Shuffle操作
			* 背景介绍
			* 对性能的影响
	* RDD持久化
		* 选择哪种存储策略?
		* 移除掉存储的数据
* 共享变量
	* 广播变量
	* 累加变量
* spark集群部署
* 使用Java或者Scala启动spark程序
* spark单元测试
* 更多内容链接

概述
---------

通常情况下，每一个spark程序都是由一个驱动程序去运行用户的 ``main`` 函数并且在集群上执行各种并行操作。一个集合的元素可以跨越集群的节点并行处理，主要是因为spark提供了一个弹性分布式数据集，简称为 ``RDD`` 。

RDDs可以使用hadoop中的文件创建(或者其他类似的文件系统)，或者使用driver程序中存在的scala集合，然后对它进行操作。

用户也可以要求spark在内存中持久化一个RDD，允许它在并行操作中有效的重用。

最后，RDDs可以自动从故障的节点中恢复。
